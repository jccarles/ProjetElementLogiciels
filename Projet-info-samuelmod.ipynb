{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements logiciels pour le traitement des données en grande dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "sparkHome = \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\"\n",
    "findspark.init(sparkHome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Petit essai pour calculer PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printSpendTime (startTime) :\n",
    "    spendTime = time.time() - startTime\n",
    "    m, s = divmod(spendTime, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print (\"-----   Temps écoulé : %dh%02dm%02ds\" % (h, m, s))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pyspark\\nimport random\\nimport time\\n\\nstartTime = time.time()\\n\\nsc = pyspark.SparkContext(appName=\"Pi\")\\nnum_samples = 100000000\\n\\ndef inside(p):     \\n  x, y = random.random(), random.random()\\n  return x*x + y*y < 1\\n\\ncount = sc.parallelize(range(0, num_samples)).filter(inside).count()\\n\\npi = 4 * count / num_samples\\nprint(pi)\\n\\nsc.stop()\\n\\nprintSpendTime(startTime)\\n'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pyspark\n",
    "import random\n",
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "printSpendTime(startTime)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre k-d classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>54.834498</td>\n",
       "      <td>36.047249</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86.635534</td>\n",
       "      <td>65.270991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>98.589105</td>\n",
       "      <td>95.628250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>94.072391</td>\n",
       "      <td>24.933514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>76.794854</td>\n",
       "      <td>8.305761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>11.230565</td>\n",
       "      <td>77.319104</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>42.726352</td>\n",
       "      <td>93.416098</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>32.864718</td>\n",
       "      <td>22.956419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>65.006222</td>\n",
       "      <td>71.735658</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75.963699</td>\n",
       "      <td>10.236305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          Y\n",
       "0  54.834498  36.047249\n",
       "1  86.635534  65.270991\n",
       "2  98.589105  95.628250\n",
       "3  94.072391  24.933514\n",
       "4  76.794854   8.305761\n",
       "5  11.230565  77.319104\n",
       "6  42.726352  93.416098\n",
       "7  32.864718  22.956419\n",
       "8  65.006222  71.735658\n",
       "9  75.963699  10.236305"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "n = 10\n",
    "data = pandas.DataFrame(np.random.rand(n,2)*100,  columns = ['X', 'Y'])\n",
    "data.iloc[0:10, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAECNJREFUeJzt3X9sXWd9x/H3d06qmjDkFtwqcbo1\nSJG7CcTcWaiQaUItk4EhaiGqFbGRVZ3yDxqFbYaEf6pJkwgy4pcmVYtaIJNQoQqWW5UJq2qK9kNa\nJAcjUghWqzLSOKExApcJrC0N3/1xjzvXvY7jexzf6+e+X5J173nuufd8dfT446PnOfdxZCaSpHL9\nVrsLkCRdXQa9JBXOoJekwhn0klQ4g16SCmfQS1Lh1gz6iPhyRFyIiKeXtV0fEU9ExDPV43VVe0TE\nlyLi2Yj4fkTcejWLlySt7Uqu6L8KvGtF20HgyczcCzxZbQO8G9hb/RwAHtiYMiVJrVoz6DPzX4Gf\nr2i+EzhaPT8KjC5r/+ds+E+gLyJ2blSxkqT129bi+27MzPMAmXk+Im6o2geA55ftd7ZqO7/yAyLi\nAI2rfnbs2PGHt9xyS4ulSFJ3Onny5M8ys3+t/VoN+tVEk7amayxk5hHgCMDw8HBOT09vcCmSVLaI\n+MmV7NfqXTcvLA3JVI8XqvazwE3L9tsNnGvxGJKkDdBq0D8G7K+e7wceXdb+4erum9uAF5eGeCRJ\n7bHm0E1EPAy8A3hDRJwF7gcOA49ExL3AGeCuavd/Ad4DPAv8GrjnKtQsSVqHNYM+Mz+4ykt3NNk3\ngY/ULUqStHH8ZqwkFW6j77qRJF2ByZk5xqdmObewyK6+XsZGBhkdGrgqxzLoJWmTTc7McWjiFIsX\nLwEwt7DIoYlTAFcl7B26kaRNNj41+3LIL1m8eInxqdmrcjyDXpI22bmFxXW112XQS9Im29XXu672\nugx6barJmTn2HT7OnoPfYt/h40zOzLW7JGnTjY0M0ru95xVtvdt7GBsZvCrHczJWm2azJ6CkTrXU\n373rRsW53ASUQa9uMzo0sGn93qEbbZrNnoCS1GDQa9Ns9gSUpAaDXptmsyegJDU4Rq9Ns9kTUJIa\nDHptqs2cgJLU4NCNJBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ\n9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDiDXpIKZ9BLUuEMekkqXK2g\nj4iPR8QPIuLpiHg4Iq6NiD0RcSIinomIb0TENRtVrCRp/VoO+ogYAD4KDGfmm4Ae4G7gM8DnM3Mv\n8Avg3o0oVJLUmrpDN9uA3ojYBrwGOA/cDhyrXj8KjNY8hiSphpaDPjPngM8CZ2gE/IvASWAhM1+q\ndjsLDNQtUpLUujpDN9cBdwJ7gF3ADuDdTXbNVd5/ICKmI2J6fn6+1TIkSWuoM3TzTuDHmTmfmReB\nCeDtQF81lAOwGzjX7M2ZeSQzhzNzuL+/v0YZkqTLqRP0Z4DbIuI1ERHAHcAPgaeAD1T77AcerVei\nJKmObWvv0lxmnoiIY8B3gZeAGeAI8C3g6xHxD1XbQxtR6EqTM3OMT81ybmGRXX29jI0MMjrkdIAk\nrdRy0ANk5v3A/SuanwPeWudz1zI5M8ehiVMsXrwEwNzCIocmTgEY9pK0wpb8Zuz41OzLIb9k8eIl\nxqdm21SRJHWuLRn05xYW19UuSd1sSwb9rr7edbVLUjfbkkE/NjJI7/aeV7T1bu9hbGSwTRVJUueq\nNRnbLksTrt51I0lr25JBD42wN9glaW1bcuhGknTlDHpJKpxBL0mFM+glqXBbdjJW6iau7aQ6DHqp\nw7m2k+py6EbqcK7tpLoMeqnDubaT6jLopQ7n2k6qy6CXOpxrO6kuJ2OlDufaTqrLoJe2ANd2Uh0O\n3UhS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TCGfSSVDjvo5dUFJd0fjWDXlIxXNK5OYduJBXDJZ2b\nM+glFcMlnZsz6CUVwyWdmzPoJRXDJZ2bczJWUjFc0rk5g15SUVzS+dUcupGkwhn0klQ4g16SCmfQ\nS1LhagV9RPRFxLGI+FFEnI6It0XE9RHxREQ8Uz1et1HFSpLWr+4V/ReBb2fmLcBbgNPAQeDJzNwL\nPFltS5LapOWgj4jXAX8MPASQmf+bmQvAncDRarejwGjdIiVJratzRf9GYB74SkTMRMSDEbEDuDEz\nzwNUjzc0e3NEHIiI6YiYnp+fr1GGJOly6gT9NuBW4IHMHAJ+xTqGaTLzSGYOZ+Zwf39/jTIkSZdT\nJ+jPAmcz80S1fYxG8L8QETsBqscL9UqUJNXRctBn5k+B5yNiabWgO4AfAo8B+6u2/cCjtSqUJNVS\nd62bvwa+FhHXAM8B99D44/FIRNwLnAHuqnkMSVINtYI+M78HDDd56Y46nytJ2jh+M1aSCmfQS1Lh\nDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6g\nl6TCGfSSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJ\nKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn0EtS4Qx6SSqcQS9Jhasd9BHRExEzEfF4tb0n\nIk5ExDMR8Y2IuKZ+mZKkVm3EFf19wOll258BPp+Ze4FfAPduwDEkSS2qFfQRsRv4U+DBajuA24Fj\n1S5HgdE6x5Ak1VP3iv4LwCeA31TbrwcWMvOlavssMNDsjRFxICKmI2J6fn6+ZhmSpNW0HPQR8V7g\nQmaeXN7cZNds9v7MPJKZw5k53N/f32oZkqQ1bKvx3n3A+yLiPcC1wOtoXOH3RcS26qp+N3CufpmS\npFa1fEWfmYcyc3dm3gzcDRzPzA8BTwEfqHbbDzxau0pJUsuuxn30nwT+JiKepTFm/9BVOIYk6QrV\nGbp5WWZ+B/hO9fw54K0b8bmSpPo2JOil0kzOzDE+Ncu5hUV29fUyNjLI6FDTG8ikjmfQSytMzsxx\naOIUixcvATC3sMihiVMAhr22JNe6kVYYn5p9OeSXLF68xPjUbJsqkuox6KUVzi0srqtd6nQGvbTC\nrr7edbVLnc6gl1YYGxmkd3vPK9p6t/cwNjLYpoqkepyMlVZYmnD1rhuVwqCXmhgdGjDYVQyHbiSp\ncAa9JBXOoJekwhn0klQ4g16SCmfQS1LhDHpJKpxBL0mFM+glqXAGvSQVzqCXpMIZ9JJUOINekgpn\n0EtS4Qx6SSqcQS9JhTPoJalw/ocpSWrB5Mzclvl3kwa9JK3T5MwchyZOsXjxEgBzC4scmjgF0JFh\n79CNJK3T+NTsyyG/ZPHiJcanZttU0eUZ9JK0TucWFtfV3m4GvSSt066+3nW1t5tB3yUmZ+bYd/g4\new5+i32HjzM5M9fukqQta2xkkN7tPa9o693ew9jIYJsqujwnY7vAVps4kjrd0u+Nd92oY1xu4qhT\nO6bU6UaHBrbM749DN11gq00cSdpYBn0X2GoTR5I2lkHfBbbaxJGkjdVy0EfETRHxVEScjogfRMR9\nVfv1EfFERDxTPV63ceWqFaNDA3z6/W9moK+XAAb6evn0+9+8ZcYXJdUTmdnaGyN2Ajsz87sR8dvA\nSWAU+Evg55l5OCIOAtdl5icv91nDw8M5PT3dUh2S1K0i4mRmDq+1X8tX9Jl5PjO/Wz3/b+A0MADc\nCRytdjtKI/wlSW2yIWP0EXEzMAScAG7MzPPQ+GMA3LDKew5ExHRETM/Pz29EGZKkJmoHfUS8Fvgm\n8LHM/OWVvi8zj2TmcGYO9/f31y1DkrSKWkEfEdtphPzXMnOian6hGr9fGse/UK9ESVIdde66CeAh\n4HRmfm7ZS48B+6vn+4FHWy9PklRXnSUQ9gF/AZyKiO9VbZ8CDgOPRMS9wBngrnolSpLqaDnoM/Pf\ngVjl5Tta/VxJ0sbym7GSVDiDXpIKZ9BLUuEMekkqnEEvSYUz6CWpcAa9JBXOoJekwhn0klQ4g16S\nCmfQS1LhDHpJKpxBL0mFM+glqXB11qOX1OUmZ+YYn5rl3MIiu/p6GRsZZHRooN1laQWDXlJLJmfm\nODRxisWLlwCYW1jk0MQpAMO+wzh0I6kl41OzL4f8ksWLlxifmm1TRVqNQS+pJecWFtfVrvYx6CW1\nZFdfb9P2BPYdPs7kzNzmFqRVGfSSWjI2Mkjv9p6mry2N1xv2ncGgl9SS0aEBPv3+NzOwypW94/Wd\nw6CX1LLRoQH+4+DtxCqvO17fGQx6SbWtNl6/Wrs2l0EvqbZm4/W923sYGxlsU0Vazi9MSapt6QtS\nfku2Mxn0kjbE6NCAwd6hHLqRpMIZ9JJUOINekgpn0EtS4Qx6SSpcZGa7ayAi5oGftLuOFrwB+Fm7\ni+hAnpdX85w053lp7krPy+9mZv9aO3VE0G9VETGdmcPtrqPTeF5ezXPSnOeluY0+Lw7dSFLhDHpJ\nKpxBX8+RdhfQoTwvr+Y5ac7z0tyGnhfH6CWpcF7RS1LhDHpJKpxBfwUi4qaIeCoiTkfEDyLivqr9\n+oh4IiKeqR6va3et7RARPRExExGPV9t7IuJEdV6+ERHXtLvGzRYRfRFxLCJ+VPWbt3V7f4mIj1e/\nP09HxMMRcW039pWI+HJEXIiIp5e1Ne0b0fCliHg2Ir4fEbe2ckyD/sq8BPxtZv4ecBvwkYj4feAg\n8GRm7gWerLa70X3A6WXbnwE+X52XXwD3tqWq9voi8O3MvAV4C43z07X9JSIGgI8Cw5n5JqAHuJvu\n7CtfBd61om21vvFuYG/1cwB4oKUjZqY/6/wBHgX+BJgFdlZtO4HZdtfWhnOxu+qYtwOPA0HjG33b\nqtffBky1u85NPievA35MdbPDsvau7S/AAPA8cD2N/4PxODDSrX0FuBl4eq2+AfwT8MFm+63nxyv6\ndYqIm4Eh4ARwY2aeB6geb2hfZW3zBeATwG+q7dcDC5n5UrV9lsYveTd5IzAPfKUa0nowInbQxf0l\nM+eAzwJngPPAi8BJ7CtLVusbS38gl7R0jgz6dYiI1wLfBD6Wmb9sdz3tFhHvBS5k5snlzU127bZ7\neLcBtwIPZOYQ8Cu6aJimmWrM+U5gD7AL2EFjWGKlbusra9mQ3yeD/gpFxHYaIf+1zJyoml+IiJ3V\n6zuBC+2qr032Ae+LiP8Cvk5j+OYLQF9ELP2byt3AufaU1zZngbOZeaLaPkYj+Lu5v7wT+HFmzmfm\nRWACeDv2lSWr9Y2zwE3L9mvpHBn0VyAiAngIOJ2Zn1v20mPA/ur5fhpj910jMw9l5u7MvJnGxNrx\nzPwQ8BTwgWq3bjwvPwWej4jBqukO4Id0d385A9wWEa+pfp+WzklX95VlVusbjwEfru6+uQ14cWmI\nZz38ZuwViIg/Av4NOMX/j0V/isY4/SPA79DoyHdl5s/bUmSbRcQ7gL/LzPdGxBtpXOFfD8wAf56Z\n/9PO+jZbRPwB8CBwDfAccA+NC6uu7S8R8ffAn9G4i20G+Csa481d1Vci4mHgHTSWIn4BuB+YpEnf\nqP4o/iONu3R+DdyTmdPrPqZBL0llc+hGkgpn0EtS4Qx6SSqcQS9JhTPoJalwBr0kFc6gl6TC/R+r\nxLpE5EYxKQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b101c97908>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data[['X']], data[['Y']], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition d'une structure de données pour les arbres binaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Classe permettant de definir un arbre binaire simple\n",
    "### Il se constitue d'un noeud racine, de deux sous-arbres\n",
    "### et des donnees correspondant au noeud racine\n",
    "class Tree(object) :\n",
    "    \n",
    "    ## Constructeur simple\n",
    "    def __init__(self) :\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.key = None\n",
    "        self.data = None\n",
    "    \n",
    "    ## Reprensation de l'arbre sous forme de chaine\n",
    "    ## de caracteres\n",
    "    def __str__(self) :\n",
    "        return self.strRec(0)\n",
    "    \n",
    "    ## Represente l'arbre sous forme de chaine de caracteres\n",
    "    ## de facon recursive\n",
    "    def strRec(self, depth) :\n",
    "        line = \"-\" * 3 * depth\n",
    "        res = line + str(self.data)\n",
    "        if self.left :\n",
    "            res += \"\\n\" + self.left.strRec(depth+1)\n",
    "        if self.right :\n",
    "            res += \"\\n\" + self.right.strRec(depth+1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "### Trace un arbre k-d (classe 'Tree') de facon recursive\n",
    "def pltTreeRec(ax, tree, keyX, keyY, xLim, yLim) :\n",
    "    if tree :\n",
    "        x, y = tree.data[keyX], tree.data[keyY]\n",
    "        if tree.key == keyX :\n",
    "            pltTreeRec(ax, tree.left, keyX, keyY, (xLim[0], x), yLim)\n",
    "            pltTreeRec(ax, tree.right, keyX, keyY, (x, xLim[1]), yLim)\n",
    "            ax.add_line(lines.Line2D((x, x), yLim, linewidth=0.5, color='red'))\n",
    "        elif tree.key == keyY :\n",
    "            pltTreeRec(ax, tree.left, keyX, keyY, xLim, (yLim[0], y))\n",
    "            pltTreeRec(ax, tree.right, keyX, keyY, xLim, (y, yLim[1]))\n",
    "            ax.add_line(lines.Line2D(xLim, (y, y), linewidth=0.5, color='red'))\n",
    "        plt.plot(x, y, 'o', color='#1f77b4')\n",
    "\n",
    "### Trace un jeu de donnees et l'arbre k-d associe.\n",
    "### Le parametre 'tree' doit etre une instance de la \n",
    "### classe 'Tree'\n",
    "def plotDataTree(data, tree) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    keyX, keyY = colList[0], colList[1]\n",
    "    ax = plt.axes()\n",
    "    plt.plot(data[['X']], data[['Y']], 'o', color='#1f77b4')\n",
    "    xLim, yLim = ax.get_xlim(), ax.get_ylim()\n",
    "    pltTreeRec(ax, tree, keyX, keyY, xLim, yLim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme \"naïf\" pour constituer un arbre k-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "On propose une première implémentation, \"naïve\", pour la constitution d'un arbre k-d. Le principe : on découpe de façon cyclique selon les différentes dimension ; à chaque étape on recherche la médiane du tableau (donc une opération de tri) selon la dimension courante ; on lui attache deux sous-arbres correspondants aux valeurs inférieures (resp. supérieures) à la médiane.\n",
    "</p>\n",
    "<p>\n",
    "D'après Bentley, si on est en mesure de trouver la médiane en $O(n)$, la complexité de l'algorithme est en $O(n\\log(n))$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDnaifRec (data, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    nRow = data.shape[0]\n",
    "    sortCol = colList[curCol]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = data.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    #if nextCol > nCol :\n",
    "    #    nextCol = 0\n",
    "    dataSort = data.sort_values(by=sortCol)\n",
    "    dataNode = dataSort.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    rootNode.left = arbreKDnaifRec(dataSort.iloc[0:median,], colList, nextCol)\n",
    "    rootNode.right = arbreKDnaifRec(dataSort.iloc[(median+1):nRow,], colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDnaif (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    curCol = 0\n",
    "    return(arbreKDnaifRec(data, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 75.96369938328323, 'Y': 10.236305278559744}\n",
      "---{'X': 65.00622182440826, 'Y': 71.7356576173022}\n",
      "------{'X': 54.83449768255173, 'Y': 36.04724860526052}\n",
      "---------{'X': 32.86471806812791, 'Y': 22.9564189285123}\n",
      "------{'X': 42.726352175866154, 'Y': 93.4160983002977}\n",
      "---------{'X': 11.23056467831718, 'Y': 77.31910361847373}\n",
      "---{'X': 86.6355342177857, 'Y': 65.2709914291312}\n",
      "------{'X': 94.07239053527006, 'Y': 24.933514044125715}\n",
      "---------{'X': 76.79485371216191, 'Y': 8.305760560900211}\n",
      "------{'X': 98.5891052240406, 'Y': 95.6282502377121}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b101f5b438>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test de la fonction 'arbreKDnaif' sur le \n",
    "### jeu de donnees 'data'\n",
    "arbre = arbreKDnaif(data)\n",
    "print(arbre)\n",
    "plotDataTree(data, arbre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Construction d'un arbre k-d avec pré-tri par dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "On implémente, toujours de façon non distribuée, un algorithme de construction d'un arbre k-d où le tri selon les différentes dimension du jeu de données est fait en amont.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDpretriRec (sortedData, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    sortCol = colList[curCol]\n",
    "    curData = sortedData[sortCol]\n",
    "    nRow = curData.shape[0]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = curData.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    dataNode = curData.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    leftSortedData = dict()\n",
    "    rightSortedData = dict()\n",
    "    for col in colList :\n",
    "        leftSortedData[col] = sortedData[col][sortedData[col][sortCol]<rootNode.data[sortCol]]\n",
    "        rightSortedData[col] = sortedData[col][sortedData[col][sortCol]>rootNode.data[sortCol]]\n",
    "    rootNode.left = arbreKDpretriRec(leftSortedData, colList, nextCol)\n",
    "    rootNode.right = arbreKDpretriRec(rightSortedData, colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDpretri (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    sortedData = dict()\n",
    "    for col in colList :\n",
    "        sortedData[col] = data.sort_values(by = col)\n",
    "    curCol = 0\n",
    "    return(arbreKDpretriRec(sortedData, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 75.96369938328323, 'Y': 10.236305278559744}\n",
      "---{'X': 65.00622182440826, 'Y': 71.7356576173022}\n",
      "------{'X': 54.83449768255173, 'Y': 36.04724860526052}\n",
      "---------{'X': 32.86471806812791, 'Y': 22.9564189285123}\n",
      "------{'X': 42.726352175866154, 'Y': 93.4160983002977}\n",
      "---------{'X': 11.23056467831718, 'Y': 77.31910361847373}\n",
      "---{'X': 86.6355342177857, 'Y': 65.2709914291312}\n",
      "------{'X': 94.07239053527006, 'Y': 24.933514044125715}\n",
      "---------{'X': 76.79485371216191, 'Y': 8.305760560900211}\n",
      "------{'X': 98.5891052240406, 'Y': 95.6282502377121}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b1020b9c50>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test de la fonction 'arbreKDpretri' sur le \n",
    "### jeu de donnees 'data'\n",
    "### --> on doit retrouver le même arbre qu'avec 'arbreKDpretri'\n",
    "arbre = arbreKDpretri(data)\n",
    "print(arbre)\n",
    "plotDataTree(data, arbre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre k-d distribué avec Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>On reprend l'algorithme de constitution d'un arbre k-d avec tri initial selon toutes les dimensions. L'idée est de ne plus charger en mémoire l'intégralité des données, ainsi que l'arbre construit. On utilise pour cela la solution Spark, basée sur les RDD.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Export du jeu de données au format CSV, pour permettre un traitement général\n",
    "data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[{'X': '54.83449768255173', 'Y': '36.04724860526052'}, {'X': '86.6355342177857', 'Y': '65.2709914291312'}, {'X': '98.5891052240406', 'Y': '95.6282502377121'}, {'X': '94.07239053527006', 'Y': '24.933514044125715'}, {'X': '76.79485371216191', 'Y': '8.305760560900211'}, {'X': '11.23056467831718', 'Y': '77.31910361847373'}, {'X': '42.726352175866154', 'Y': '93.4160983002977'}, {'X': '32.86471806812791', 'Y': '22.9564189285123'}, {'X': '65.00622182440826', 'Y': '71.7356576173022'}, {'X': '75.96369938328323', 'Y': '10.236305278559744'}]\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h00m30s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Chargement du RDD (dans un fichier CSV)\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "def loadRecord(line, header) :\n",
    "    values = line.split(\",\")\n",
    "    res = dict()\n",
    "    for i in range(0, len(header)) :\n",
    "        res[header[i]] = values[i]\n",
    "    return res\n",
    "\n",
    "def csvToRdd(sc, file) :\n",
    "    rddFile = sc.textFile(file)\n",
    "    rawHeader = rddFile.first()\n",
    "    header = rawHeader.split(\",\")\n",
    "    rddRows = rddFile.filter(lambda l : l != rawHeader).map(lambda l : loadRecord(l, header))\n",
    "    return rddRows\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Essai\")\n",
    "\n",
    "rdd = csvToRdd(sc, \"data.csv\")\n",
    "print(rdd.count())\n",
    "print(rdd.take(10))\n",
    "\n",
    "#sc.stop()\n",
    "\n",
    "printSpendTime(startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5f8ddc4d3519>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = iter(range(0, 3))\n",
    "print(next(test))\n",
    "print(next(test))\n",
    "print(next(test))\n",
    "print(next(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Constitution d'un arbre KD avec Spark\n",
      "Entrée : data.csv - Sortie : arbre\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h00m10s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import time\n",
    "\n",
    "def loadRecord(line, header, col) :\n",
    "    values = line.split(\",\")\n",
    "    res = dict()\n",
    "    for i in range(0, len(header)) :\n",
    "        res[header[i]] = values[i]\n",
    "    return (res[col], res)\n",
    "\n",
    "def createRddDict(sc, file) :\n",
    "    rddFile = sc.textFile(file)\n",
    "    rawHeader = rddFile.first()\n",
    "    header = rawHeader.split(\",\")\n",
    "    rawRows = rddFile.filter(lambda l : l != rawHeader)\n",
    "    res = dict()\n",
    "    for col in header :\n",
    "        res[col] = rawRows.map(lambda l : loadRecord(l, header, col)).sortByKey()\n",
    "    return res\n",
    "\n",
    "def arbreKDsparkRec (sortedData, colList, curCol) :\n",
    "    ### Implementer\n",
    "    return \"\"\n",
    "\n",
    "def arbreKDspark (inputFile, outputFile) :\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"Constitution d'un arbre KD avec Spark\")\n",
    "    print(\"Entrée : \" + inputFile + \" - Sortie : \" + outputFile)\n",
    "    startTime = time.time()\n",
    "    \n",
    "    sc = pyspark.SparkContext(appName=\"arbreKD\")\n",
    "    \n",
    "    rddDict = createRddDict(sc, inputFile)\n",
    "    colList = list(rddDict.keys())\n",
    "    res = arbreKDsparkRec(rddDict, colList, 0)\n",
    "    \n",
    "    sc.stop()\n",
    "    \n",
    "    printSpendTime(startTime)\n",
    "    return res\n",
    "\n",
    "arbre = arbreKDspark(\"data.csv\", \"arbre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[{'X': '11.9264091132209', 'Y': '57.1213151095435'}, {'X': '66.3763535907492', 'Y': '50.5452962825075'}, {'X': '92.697916016914', 'Y': '0.581420445814729'}, {'X': '9.23256054520607', 'Y': '75.2423814730719'}, {'X': '29.2442676145583', 'Y': '49.9356162501499'}, {'X': '34.9739615339786', 'Y': '28.3505846280605'}, {'X': '73.1766821118072', 'Y': '69.346927292645'}, {'X': '27.1010003052652', 'Y': '51.4135136036202'}, {'X': '74.4333170354366', 'Y': '70.2033943729475'}, {'X': '59.8525910638273', 'Y': '60.7070606900379'}]\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h00m04s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Chargement du RDD (dans un fichier CSV)\n",
    "# import csv\n",
    "# import StringIO\n",
    "\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Essai\")\n",
    "dataFile = \"C:/Users/carle/OneDrive/Documents/GitHub/ProjetElementLogiciels/dataFile.csv\"\n",
    "\n",
    "def loadRecord(line) :\n",
    "    res = line.split(\",\")\n",
    "    res = {\"X\":res[0],\"Y\":res[1]}\n",
    "    return res\n",
    "\n",
    "rdd = sc.textFile(dataFile).map(loadRecord)\n",
    "print(rdd.count())\n",
    "print(rdd.take(10))\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "printSpendTime(startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = (1,{\"x\":1,\"y\":2})\n",
    "print(test[1][\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDsparkRec (sortedData, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    sortCol = colList[curCol]\n",
    "    curData = sortedData[sortCol]\n",
    "    nRow = curData.shape[0]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = curData.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    dataNode = curData.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    leftSortedData = dict()\n",
    "    rightSortedData = dict()\n",
    "    for col in colList :\n",
    "        leftSortedData[col] = sortedData[col][sortedData[col][sortCol]<rootNode.data[sortCol]]\n",
    "        rightSortedData[col] = sortedData[col][sortedData[col][sortCol]>rootNode.data[sortCol]]\n",
    "    rootNode.left = arbreKDpretriRec(leftSortedData, colList, nextCol)\n",
    "    rootNode.right = arbreKDpretriRec(rightSortedData, colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDspark (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    sortedData = dict()\n",
    "    for col in colList :\n",
    "        sortedData[col] = data.sort_values(by = col)\n",
    "    curCol = 0\n",
    "    return(arbreKDsparkRec(sortedData, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function repr in module builtins:\n",
      "\n",
      "repr(obj, /)\n",
      "    Return the canonical string representation of the object.\n",
      "    \n",
      "    For many object types, including most builtins, eval(repr(obj)) == obj.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.3574285286\n",
      "57.357428528562714\n"
     ]
    }
   ],
   "source": [
    "print(data[\"X\"][1])\n",
    "print(repr(data[\"X\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X': '29.444807451657095', 'Y': '66.86872788284245'},\n",
       " {'X': '45.173565574844865', 'Y': '25.15052435089181'},\n",
       " {'X': '83.26567848382234', 'Y': '54.5358545818022'},\n",
       " {'X': '56.13155610717935', 'Y': '40.182397178675345'},\n",
       " {'X': '52.31221096817371', 'Y': '3.741651998050566'},\n",
       " {'X': '23.044419715033314', 'Y': '5.770603777422966'},\n",
       " {'X': '1.487048092245702', 'Y': '44.19470454641951'},\n",
       " {'X': '7.246164719807913', 'Y': '15.007920319739043'},\n",
       " {'X': '26.615822471050798', 'Y': '24.66700415557126'},\n",
       " {'X': '75.02430718936269', 'Y': '36.567030693720135'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 5.0 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getMedian (rdd_sorted) :\n",
    "    l = rdd.glom().map(len).collect()  # get length of each partition\n",
    "\n",
    "    n = sum(l) #number of elements in all partitions\n",
    "    median = int(n / 2)\n",
    "    if n % 2 == 1 :\n",
    "        median = int((n-1) / 2)\n",
    "    cumsuml = np.cumsum(l) - median\n",
    "    filtre = [x for x in cumsuml if x>=0] #filter partition with element smallers than median\n",
    "    partition_median = len(l)-len(filtre) #partition where the median lye\n",
    "    \n",
    "    return(rdd.glom().collect()[partition_median][filtre[0]]) #return the median element (X : Xvalue, Y : Yvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 10], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "True is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-803b3e61afed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: True is not in list"
     ]
    }
   ],
   "source": [
    "l.index(min((np.cumsum(l)>=(rdd.count()//2))!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "import time\n",
    "\n",
    "def median(rdd_sorted):\n",
    "    \"\"\"Compute the median\n",
    "    :rdd a numeric rdd\n",
    "    \"\"\"\n",
    "    n = rdd_sorted.count() #compute the number of elements of the RDD\n",
    "    h = (n - 1) * 0.5\n",
    "\n",
    "    rddmedian, rddmedianPlusOne = (rdd_sorted.lookup(x)[0] for x in (floor(h)) + np.array([0, 1]))\n",
    "\n",
    "    return(rddmedian + (h - floor(h)) * (rddXPlusOne - rddX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'54.83449768255173'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.first()[\"X\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 33, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-9a1ad60db69d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 33, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': '26.615822471050798', 'Y': '24.66700415557126'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X': '29.444807451657095', 'Y': '66.86872788284245'},\n",
       " {'X': '45.173565574844865', 'Y': '25.15052435089181'},\n",
       " {'X': '83.26567848382234', 'Y': '54.5358545818022'},\n",
       " {'X': '56.13155610717935', 'Y': '40.182397178675345'},\n",
       " {'X': '52.31221096817371', 'Y': '3.741651998050566'},\n",
       " {'X': '23.044419715033314', 'Y': '5.770603777422966'},\n",
       " {'X': '1.487048092245702', 'Y': '44.19470454641951'},\n",
       " {'X': '7.246164719807913', 'Y': '15.007920319739043'},\n",
       " {'X': '26.615822471050798', 'Y': '24.66700415557126'},\n",
       " {'X': '75.02430718936269', 'Y': '36.567030693720135'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = (-6,-4,-2,1,2,3,5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6, -4, -2, 1, 2, 3, 5, 8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "liste = [x for x in a if x>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 8]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
