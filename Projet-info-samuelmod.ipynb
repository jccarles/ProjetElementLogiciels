{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements logiciels pour le traitement des données en grande dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "sparkHome = \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\"\n",
    "findspark.init(sparkHome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Petit essai pour calculer PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printSpendTime (startTime) :\n",
    "    spendTime = time.time() - startTime\n",
    "    m, s = divmod(spendTime, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print (\"-----   Temps écoulé : %dh%02dm%02ds\" % (h, m, s))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport pyspark\\nimport random\\nimport time\\n\\nstartTime = time.time()\\n\\nsc = pyspark.SparkContext(appName=\"Pi\")\\nnum_samples = 100000000\\n\\ndef inside(p):     \\n  x, y = random.random(), random.random()\\n  return x*x + y*y < 1\\n\\ncount = sc.parallelize(range(0, num_samples)).filter(inside).count()\\n\\npi = 4 * count / num_samples\\nprint(pi)\\n\\nsc.stop()\\n\\nprintSpendTime(startTime)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "import pyspark\n",
    "import random\n",
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "printSpendTime(startTime)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre k-d classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>29.444807</td>\n",
       "      <td>66.868728</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>45.173566</td>\n",
       "      <td>25.150524</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>83.265678</td>\n",
       "      <td>54.535855</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>56.131556</td>\n",
       "      <td>40.182397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>52.312211</td>\n",
       "      <td>3.741652</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>23.044420</td>\n",
       "      <td>5.770604</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1.487048</td>\n",
       "      <td>44.194705</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>7.246165</td>\n",
       "      <td>15.007920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>26.615822</td>\n",
       "      <td>24.667004</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>75.024307</td>\n",
       "      <td>36.567031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          Y\n",
       "0  29.444807  66.868728\n",
       "1  45.173566  25.150524\n",
       "2  83.265678  54.535855\n",
       "3  56.131556  40.182397\n",
       "4  52.312211   3.741652\n",
       "5  23.044420   5.770604\n",
       "6   1.487048  44.194705\n",
       "7   7.246165  15.007920\n",
       "8  26.615822  24.667004\n",
       "9  75.024307  36.567031"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "n = 10\n",
    "data = pandas.DataFrame(np.random.rand(n,2)*100,  columns = ['X', 'Y'])\n",
    "data.iloc[0:10, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18bfb6732b0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data[['X']], data[['Y']], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition d'une structure de données pour les arbres binaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Classe permettant de definir un arbre binaire simple\n",
    "### Il se constitue d'un noeud racine, de deux sous-arbres\n",
    "### et des donnees correspondant au noeud racine\n",
    "class Tree(object) :\n",
    "    \n",
    "    ## Constructeur simple\n",
    "    def __init__(self) :\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.key = None\n",
    "        self.data = None\n",
    "    \n",
    "    ## Reprensation de l'arbre sous forme de chaine\n",
    "    ## de caracteres\n",
    "    def __str__(self) :\n",
    "        return self.strRec(0)\n",
    "    \n",
    "    ## Represente l'arbre sous forme de chaine de caracteres\n",
    "    ## de facon recursive\n",
    "    def strRec(self, depth) :\n",
    "        line = \"-\" * 3 * depth\n",
    "        res = line + str(self.data)\n",
    "        if self.left :\n",
    "            res += \"\\n\" + self.left.strRec(depth+1)\n",
    "        if self.right :\n",
    "            res += \"\\n\" + self.right.strRec(depth+1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "### Trace un arbre k-d (classe 'Tree') de facon recursive\n",
    "def pltTreeRec(ax, tree, keyX, keyY, xLim, yLim) :\n",
    "    if tree :\n",
    "        x, y = tree.data[keyX], tree.data[keyY]\n",
    "        if tree.key == keyX :\n",
    "            pltTreeRec(ax, tree.left, keyX, keyY, (xLim[0], x), yLim)\n",
    "            pltTreeRec(ax, tree.right, keyX, keyY, (x, xLim[1]), yLim)\n",
    "            ax.add_line(lines.Line2D((x, x), yLim, linewidth=0.5, color='red'))\n",
    "        elif tree.key == keyY :\n",
    "            pltTreeRec(ax, tree.left, keyX, keyY, xLim, (yLim[0], y))\n",
    "            pltTreeRec(ax, tree.right, keyX, keyY, xLim, (y, yLim[1]))\n",
    "            ax.add_line(lines.Line2D(xLim, (y, y), linewidth=0.5, color='red'))\n",
    "        plt.plot(x, y, 'o', color='#1f77b4')\n",
    "\n",
    "### Trace un jeu de donnees et l'arbre k-d associe.\n",
    "### Le parametre 'tree' doit etre une instance de la \n",
    "### classe 'Tree'\n",
    "def plotDataTree(data, tree) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    keyX, keyY = colList[0], colList[1]\n",
    "    ax = plt.axes()\n",
    "    plt.plot(data[['X']], data[['Y']], 'o', color='#1f77b4')\n",
    "    xLim, yLim = ax.get_xlim(), ax.get_ylim()\n",
    "    pltTreeRec(ax, tree, keyX, keyY, xLim, yLim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme \"naïf\" pour constituer un arbre k-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "On propose une première implémentation, \"naïve\", pour la constitution d'un arbre k-d. Le principe : on découpe de façon cyclique selon les différentes dimension ; à chaque étape on recherche la médiane du tableau (donc une opération de tri) selon la dimension courante ; on lui attache deux sous-arbres correspondants aux valeurs inférieures (resp. supérieures) à la médiane.\n",
    "</p>\n",
    "<p>\n",
    "D'après Bentley, si on est en mesure de trouver la médiane en $O(n)$, la complexité de l'algorithme est en $O(n\\log(n))$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDnaifRec (data, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    nRow = data.shape[0]\n",
    "    sortCol = colList[curCol]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = data.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    #if nextCol > nCol :\n",
    "    #    nextCol = 0\n",
    "    dataSort = data.sort_values(by=sortCol)\n",
    "    dataNode = dataSort.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    rootNode.left = arbreKDnaifRec(dataSort.iloc[0:median,], colList, nextCol)\n",
    "    rootNode.right = arbreKDnaifRec(dataSort.iloc[(median+1):nRow,], colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDnaif (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    curCol = 0\n",
    "    return(arbreKDnaifRec(data, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 45.173565574844865, 'Y': 25.15052435089181}\n",
      "---{'X': 26.615822471050798, 'Y': 24.66700415557126}\n",
      "------{'X': 23.044419715033314, 'Y': 5.770603777422966}\n",
      "---------{'X': 7.246164719807913, 'Y': 15.007920319739043}\n",
      "------{'X': 29.444807451657095, 'Y': 66.86872788284245}\n",
      "---------{'X': 1.487048092245702, 'Y': 44.19470454641951}\n",
      "---{'X': 56.13155610717935, 'Y': 40.182397178675345}\n",
      "------{'X': 75.02430718936269, 'Y': 36.567030693720135}\n",
      "---------{'X': 52.31221096817371, 'Y': 3.741651998050566}\n",
      "------{'X': 83.26567848382234, 'Y': 54.5358545818022}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMS4wLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvpW3flQAAEqRJREFUeJzt3W9snWd5x/HvZZcOKGxtqFtlTTqX\nKCpUi5oyqwnrhFhLJxgd9Qu6NaIoQZ3yhm1lYmIN8osgZVKRJv68mJCiFhKpLKUrxalQxFaFom3S\n6uG23kwJKNCFJDQ0BlpgUNHVvvbiPE7zx+l5jn2Oj899vh/JOue+z3NyLj9+8vPt+/kXmYkkqfcN\ndLsASVJ7GOiSVAgDXZIKYaBLUiEMdEkqhIEuSYUw0CWpEAa6JBXCQJekQlywnB926aWX5vDw8HJ+\npCT1vCeeeOLHmTnUbLllDfTh4WEmJyeX8yMlqedFxA/qLOeUiyQVwkCXpEIY6JJUCANdkgphoEtS\nIZoGekRcHRFTp339PCI+EhGrIuLRiDhcPV6yHAVLkhbWNNAz87uZuTEzNwK/B/wK+ApwN3AwM9cD\nB6u2JKlLWp1yuQn4fmb+ALgV2Fv17wVG21mYJKk1rQb67cC+6vnlmXkCoHq8bKE3RMT2iJiMiMmZ\nmZnFV6resmdPtyvojFK/L9W3greB2oEeERcC7wP+qZUPyMzdmTmSmSNDQ03PXFUpjhzpdgWdUer3\npfpW8DbQygj9PcCTmflc1X4uIlYDVI8n212cJKm+VgJ9C69MtwA8Amytnm8F9rerKElS62oFekS8\nHrgZePi07nuAmyPicPXaPe0vT5JUV62rLWbmr4A3ndX3ExpHvUiSVgDPFFVbjY1Ps27HAYZfHGHd\njgOMjU93uySpbyzr9dBVtrHxae5//GijEcFs5qn2rtENXaxM6g+O0NU2+yaOtdQvqb0MdLXNbGZL\n/ZLay0BX2wxGtNQvqb0MdLXNlk1rW+qX1F7uFFXbzO/43DdxjNm5OQYHBtiyaa07RKVlYqCrrXaN\nbmgE+M6djS9Jy8YpF0kqhIEuSYUw0CWpEAa6JBXCQJekQhjoklQIA12SCmGgS1IhDHRJKoSBLkmF\nMNAlqRB1bxJ9cUQ8FBHfiYhDEfH2iFgVEY9GxOHq8ZJOFytJOr+6I/TPAl/LzLcA1wKHgLuBg5m5\nHjhYtSVJXdI00CPiN4F3APcBZOZLmfkCcCuwt1psLzDaqSIlSc3VGaG/GZgBvhART0XEvRFxEXB5\nZp4AqB4v62CdkqQm6gT6BcDbgM9l5nXAL2lheiUitkfEZERMzszMLLJMSVIzdQL9OHA8Myeq9kM0\nAv65iFgNUD2eXOjNmbk7M0cyc2RoaKgdNUuSFtA00DPzR8CxiLi66roJ+DbwCLC16tsK7O9IhZKk\nWuregu4vgS9GxIXAM8CHaPwyeDAi7gSOArd1pkRJUh21Aj0zp4CRBV66qb3lSJIWyzNFJakQBrok\nFcJAl6RCGOiSVAgDXZIKYaBLUiEMdEkqhIEuSTWMjU+zbscBhl8cYd2OA4yNT3e7pHPUPVNUkvrW\n2Pg09z9+tNGIYDbzVHvX6IYuVnYmR+iS1MS+iWMt9XeLgS5JTcxmttTfLQa6JDUxGNFSf7cY6JLU\nxJZNa1vq7xZ3ikpSE/M7PvdNHGN2bo7BgQG2bFq7onaIgoEuSbXsGt3QCPCdOxtfK5BTLpJUCANd\nkgphoEtSIQx0SSqEgS5Jhah1lEtEHAF+AcwCL2fmSESsAr4EDANHgD/NzOc7U6YkqZlWRuh/mJkb\nM3Okat8NHMzM9cDBqi1J6pKlTLncCuytnu8FRpdejiRpseoGegL/EhFPRMT2qu/yzDwBUD1ettAb\nI2J7RExGxOTMzMzSK5YkLajumaI3ZOazEXEZ8GhEfKfuB2TmbmA3wMjIyMq6NJkkFaTWCD0zn60e\nTwJfAa4HnouI1QDV48lOFSlJaq5poEfERRHxxvnnwB8B3wIeAbZWi20F9neqSElSc3WmXC4HvhKN\n6/5eAPxjZn4tIr4JPBgRdwJHgds6V6YkqZmmgZ6ZzwDXLtD/E+CmThQlSWqdZ4pKUiEMdEkqhIEu\nSYUw0CWpEAa6JBXCQJekQhjoklQIA12SClH34lw9Z2x8mn0Tx5jNZDCCLZvWsmt0Q7fLkqSOKTLQ\nx8anuf/xo6fas5mn2oa6pFIVOeWyb+JYS/2SVILeGaHv2QNHjtRadHZuBBoXEzurfw527mxrWTqP\nqaluVyD1nd4J9G3bai86uOMAs3nuvTQGBwYM9OXiepaWXZFTLls2rW2pX5JK0Dsj9BbM7/j0KBdJ\n/aTIQIdGqBvgkvpJkVMuktSPDHRJKoSBLkmFqD2HHhGDwCTww8y8JSKuAh4AVgFPAh/MzJc6U6ak\ns3l5C52tlRH6XcCh09qfBD6dmeuB54E721mYpPObv7zF/PkW85e3GBuf7nJl6qZagR4Ra4D3AvdW\n7QBuBB6qFtkLjHaiQEnn8vIWWkjdKZfPAB8D3li13wS8kJkvV+3jwBVtrk1aeaamVsRZsF7e4jRT\nU7Bx4/J+XqvreHi4pbPdF6tpoEfELcDJzHwiIt45373Aoueea994/3ZgO8CVV165yDKlFWLjxhUR\nmF7e4jQ7d/bf93wedaZcbgDeFxFHaOwEvZHGiP3iiJj/hbAGeHahN2fm7swcycyRoaGhNpQsyctb\naCFNAz0zd2TmmswcBm4Hvp6ZHwAeA95fLbYV2N+xKiWdYdfoBu7YfCWD1bTLYAR3bL7So1z63FJO\n/f9b4IGI2AU8BdzXnpIk1eHlLXS2lgI9M78BfKN6/gxwfftLkiQthmeKSlIhDHRJPWlsfJp1Ow4w\n/OII63Yc8KQqCr58rqRynXEj+AhvBF9xhC6p53im7MIMdEk9Z6GTql6tv18Y6JJ6zuAClz14tf5+\nYaBL6jmeKbswd4pK6jln3Ah+bo7BgQGvB4+BLqlHnTpT1otzneKUiyQVwkCXpEIY6JJUCANdkgph\noEtSIQx0SSqEgS5JhTDQJakQBrokFcJAl6RCGOiSVIimgR4Rr42I/4yI/4qIpyPiE1X/VRExERGH\nI+JLEXFh58uVJJ1PnRH6r4EbM/NaYCPw7ojYDHwS+HRmrgeeB+7sXJmSpGaaBno2/G/VfE31lcCN\nwENV/15gtCMVSpJqqTWHHhGDETEFnAQeBb4PvJCZL1eLHAeuOM97t0fEZERMzszMtKNmSdICagV6\nZs5m5kZgDXA98NaFFjvPe3dn5khmjgwNDS2+UknSq2rpKJfMfAH4BrAZuDgi5m+QsQZ4tr2lSZJa\nUecol6GIuLh6/jrgXcAh4DHg/dViW4H9nSpSktRcnVvQrQb2RsQgjV8AD2bmVyPi28ADEbELeAq4\nr4N1SpKaaBromfnfwHUL9D9DYz5dkrQCeKaoJBXCQJekQhjoklQIA12SCmGgS1IhDHSphrHxadbt\nOMDwiyOs23GAsfHpbpcknaPOcegS0Ai1fRPHmM1kMIItm9aya3RDt8vquLHxae5//GijEcFs5ql2\nP3z/6h2O0FXLfKjNZuOSPfOh1g8j1X0Tx1rql7rFQFct/Rxq87/E6vZL3dI7Uy579sCRI92uokxT\nU7Bx46suMjs3AhEL9M/Bzp0L/5sL9feg3/r1NfzsNy46p39wgfUhdVPvBPq2bd2uoFw7dzYN38Ed\nBxYckQ4ODBQT3OfzJ6fPoZ9my6a1XahGOj+nXFTL+cKrH0Jt1+gG7th8ZWNEXu0QvmPzle4Q1YrT\nOyN0ddV8ePXjUS7Q+P53jW6o9deM1C0Gumo7FWqSViSnXCSpEAa6JBXCQJekQhjoklQIA12SCtE0\n0CNibUQ8FhGHIuLpiLir6l8VEY9GxOHq8ZLOlytJOp86I/SXgY9m5luBzcCHI+Ia4G7gYGauBw5W\nbUlSlzQN9Mw8kZlPVs9/ARwCrgBuBfZWi+0FRjtVpCSpuZbm0CNiGLgOmAAuz8wT0Ah94LJ2FydJ\nqq92oEfEG4AvAx/JzJ+38L7tETEZEZMzMzOLqVGSVEOtQI+I19AI8y9m5sNV93MRsbp6fTVwcqH3\nZubuzBzJzJGhoaF21CxJWkCdo1wCuA84lJmfOu2lR4Ct1fOtwP72lydJqqvOxbluAD4ITEfEVNX3\nceAe4MGIuBM4CtzWmRIlSXU0DfTM/HfgfLdmuam95UiSFsszRSWpEAa6JBXCQJekQhjoklQIA12S\nCmGgS1IhvEl0DWPj0317t3tJvcNAb2JsfJr7Hz96qj2beaptqEtaSZxyaWLfxLGW+iWpW3pnhL5n\nDxw5suwfOzs3AnHuibKzc3Owc+ey19MRU1PNl5G04vVOoG/b1pWPHdxxgNnMc/sHBsoJ9FK+D6nP\nOeXSxJZNa1vql6Ru6Z0RepfM7/j0KBdJK52BXsOu0Q0GuKQVzykXSSqEgS5JhTDQJakQBrokFcJA\nl6RCNA30iPh8RJyMiG+d1rcqIh6NiMPV4yWdLVOS1EydEfoe4N1n9d0NHMzM9cDBqi1J6qKmgZ6Z\n/wr89KzuW4G91fO9wGib65IktWixc+iXZ+YJgOrxsvaVJElajI7vFI2I7RExGRGTMzMznf44Sepb\niw305yJiNUD1ePJ8C2bm7swcycyRoaGhRX6cJKmZxQb6I8DW6vlWYH97ypEkLVadwxb3Af8BXB0R\nxyPiTuAe4OaIOAzcXLUlSV3U9GqLmbnlPC/d1OZaJElL4JmiklQIA12SCmGgS1IhDHRJKoSB3sfG\nxqdZt+MAwy+OsG7HAcbGp7tdkqQl8J6ifWpsfJr7Hz/aaEQwm3mq7f1Tpd7kCL1P7Zs41lK/pJXP\nQO9Ts5kt9Uta+Qz0PjUY0VK/pJXPQO9TWzatbalf0srnTtE+Nb/jc9/EMWbn5hgcGGDLprXuEO2S\nsfHpxs8ik8EIfxZaFAO9j+0a3dAIjZ07G1/qijOOOAKPONKiOeUidZlHHKldHKFLrZiaavtfM7Nz\nI7DAzujZuTn/cqpjaqrbFawYBrrUio0b2x6ygzsOLHi46ODAgIFeh+voFKdcpC7ziCO1iyN0qcvO\nOOLIo1y0BAa6tAKcOuJIWgKnXCSpEAa6JBViSYEeEe+OiO9GxPci4u52FSVJat2iAz0iBoF/AN4D\nXANsiYhr2lWYJKk1SxmhXw98LzOfycyXgAeAW9tTlpbV8HC3K+gdrquVx5/JKUsJ9CuA089NPl71\nqdds29btCnqH62rl8WdyylICfaELZ59zultEbI+IyYiYnJmZWcLHSZJezVIC/Thw+qlsa4Bnz14o\nM3dn5khmjgwNDS3h4yRJr2Ypgf5NYH1EXBURFwK3A4+0pyxJUqsWfaZoZr4cEX8B/DMwCHw+M59u\nW2WSpJYs6dT/zDwAHGhTLZKkJfBMUUkqhIEuSYUw0CWpEJEL3CmlYx8WMQP8oIMfcSnw4w7++73G\n9fEK18WZXB9nWunr43cys+lx38sa6J0WEZOZOdLtOlYK18crXBdncn2cqZT14ZSLJBXCQJekQpQW\n6Lu7XcAK4/p4heviTK6PMxWxPoqaQ5ekflbaCF2S+lYRgd7vt8KLiLUR8VhEHIqIpyPirqp/VUQ8\nGhGHq8dLul3rcomIwYh4KiK+WrWvioiJal18qbqgXF+IiIsj4qGI+E61jby9z7eNv67+n3wrIvZF\nxGtL2T56PtC9FR4ALwMfzcy3ApuBD1fr4G7gYGauBw5W7X5xF3DotPYngU9X6+J54M6uVNUdnwW+\nlplvAa6lsV76ctuIiCuAvwJGMvN3aVxY8HYK2T56PtDxVnhk5onMfLJ6/gsa/2GvoLEe9laL7QVG\nu1Ph8oqINcB7gXurdgA3Ag9Vi/TTuvhN4B3AfQCZ+VJmvkCfbhuVC4DXRcQFwOuBExSyfZQQ6N4K\n7zQRMQxcB0wAl2fmCWiEPnBZ9ypbVp8BPgbMVe03AS9k5stVu5+2kTcDM8AXqimoeyPiIvp028jM\nHwJ/DxylEeQ/A56gkO2jhECvdSu8fhARbwC+DHwkM3/e7Xq6ISJuAU5m5hOndy+waL9sIxcAbwM+\nl5nXAb+kT6ZXFlLtK7gVuAr4beAiGtO1Z+vJ7aOEQK91K7zSRcRraIT5FzPz4ar7uYhYXb2+GjjZ\nrfqW0Q3A+yLiCI3ptxtpjNgvrv7Ehv7aRo4DxzNzomo/RCPg+3HbAHgX8D+ZOZOZ/wc8DPw+hWwf\nJQR6398Kr5ojvg84lJmfOu2lR4Ct1fOtwP7lrm25ZeaOzFyTmcM0toWvZ+YHgMeA91eL9cW6AMjM\nHwHHIuLqqusm4Nv04bZROQpsjojXV/9v5tdHEdtHEScWRcQf0xiFzd8K7++6XNKyiog/AP4NmOaV\neeOP05hHfxC4ksaGfFtm/rQrRXZBRLwT+JvMvCUi3kxjxL4KeAq4IzN/3c36lktEbKSxg/hC4Bng\nQzQGc325bUTEJ4A/o3F02FPAn9OYM+/57aOIQJcklTHlIknCQJekYhjoklQIA12SCmGgS1IhDHRJ\nKoSBLkmFMNAlqRD/D0f5qk1qI/GOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18bfb9bd160>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test de la fonction 'arbreKDnaif' sur le \n",
    "### jeu de donnees 'data'\n",
    "arbre = arbreKDnaif(data)\n",
    "print(arbre)\n",
    "plotDataTree(data, arbre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Construction d'un arbre k-d avec pré-tri par dimension"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "On implémente, toujours de façon non distribuée, un algorithme de construction d'un arbre k-d où le tri selon les différentes dimension du jeu de données est fait en amont.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDpretriRec (sortedData, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    sortCol = colList[curCol]\n",
    "    curData = sortedData[sortCol]\n",
    "    nRow = curData.shape[0]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = curData.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    dataNode = curData.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    leftSortedData = dict()\n",
    "    rightSortedData = dict()\n",
    "    for col in colList :\n",
    "        leftSortedData[col] = sortedData[col][sortedData[col][sortCol]<rootNode.data[sortCol]]\n",
    "        rightSortedData[col] = sortedData[col][sortedData[col][sortCol]>rootNode.data[sortCol]]\n",
    "    rootNode.left = arbreKDpretriRec(leftSortedData, colList, nextCol)\n",
    "    rootNode.right = arbreKDpretriRec(rightSortedData, colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDpretri (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    sortedData = dict()\n",
    "    for col in colList :\n",
    "        sortedData[col] = data.sort_values(by = col)\n",
    "    curCol = 0\n",
    "    return(arbreKDpretriRec(sortedData, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 45.173565574844865, 'Y': 25.15052435089181}\n",
      "---{'X': 26.615822471050798, 'Y': 24.66700415557126}\n",
      "------{'X': 23.044419715033314, 'Y': 5.770603777422966}\n",
      "---------{'X': 7.246164719807913, 'Y': 15.007920319739043}\n",
      "------{'X': 29.444807451657095, 'Y': 66.86872788284245}\n",
      "---------{'X': 1.487048092245702, 'Y': 44.19470454641951}\n",
      "---{'X': 56.13155610717935, 'Y': 40.182397178675345}\n",
      "------{'X': 75.02430718936269, 'Y': 36.567030693720135}\n",
      "---------{'X': 52.31221096817371, 'Y': 3.741651998050566}\n",
      "------{'X': 83.26567848382234, 'Y': 54.5358545818022}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXQAAAD8CAYAAABn919SAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nKoSBLkmFMNAlqRD/D0f5qk1qI/GOAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x18bfa94fd30>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test de la fonction 'arbreKDpretri' sur le \n",
    "### jeu de donnees 'data'\n",
    "### --> on doit retrouver le même arbre qu'avec 'arbreKDpretri'\n",
    "arbre = arbreKDpretri(data)\n",
    "print(arbre)\n",
    "plotDataTree(data, arbre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre k-d distribué avec Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>On reprend l'algorithme de constitution d'un arbre k-d avec tri initial selon toutes les dimensions. L'idée est de ne plus charger en mémoire l'intégralité des données, ainsi que l'arbre construit. On utilise pour cela la solution Spark, basée sur les RDD.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Export du jeu de données au format CSV, pour permettre un traitement général\n",
    "data.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n",
      "[{'X': '29.444807451657095', 'Y': '66.86872788284245'}, {'X': '45.173565574844865', 'Y': '25.15052435089181'}, {'X': '83.26567848382234', 'Y': '54.5358545818022'}, {'X': '56.13155610717935', 'Y': '40.182397178675345'}, {'X': '52.31221096817371', 'Y': '3.741651998050566'}, {'X': '23.044419715033314', 'Y': '5.770603777422966'}, {'X': '1.487048092245702', 'Y': '44.19470454641951'}, {'X': '7.246164719807913', 'Y': '15.007920319739043'}, {'X': '26.615822471050798', 'Y': '24.66700415557126'}, {'X': '75.02430718936269', 'Y': '36.567030693720135'}]\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h00m07s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Chargement du RDD (dans un fichier CSV)\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "def loadRecord(line, header) :\n",
    "    values = line.split(\",\")\n",
    "    res = dict()\n",
    "    for i in range(0, len(header)) :\n",
    "        res[header[i]] = values[i]\n",
    "    return res\n",
    "\n",
    "def csvToRdd(sc, file) :\n",
    "    rddFile = sc.textFile(file)\n",
    "    rawHeader = rddFile.first()\n",
    "    header = rawHeader.split(\",\")\n",
    "    rddRows = rddFile.filter(lambda l : l != rawHeader).map(lambda l : loadRecord(l, header))\n",
    "    return rddRows\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Essai\")\n",
    "\n",
    "rdd = csvToRdd(sc, \"data.csv\")\n",
    "print(rdd.count())\n",
    "print(rdd.take(10))\n",
    "\n",
    "#sc.stop()\n",
    "\n",
    "printSpendTime(startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n"
     ]
    },
    {
     "ename": "StopIteration",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-5f8ddc4d3519>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mStopIteration\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test = iter(range(0, 3))\n",
    "print(next(test))\n",
    "print(next(test))\n",
    "print(next(test))\n",
    "print(next(test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Constitution d'un arbre KD avec Spark\n",
      "Entrée : data.csv - Sortie : arbre\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h00m10s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "import time\n",
    "\n",
    "def loadRecord(line, header, col) :\n",
    "    values = line.split(\",\")\n",
    "    res = dict()\n",
    "    for i in range(0, len(header)) :\n",
    "        res[header[i]] = values[i]\n",
    "    return (res[col], res)\n",
    "\n",
    "def createRddDict(sc, file) :\n",
    "    rddFile = sc.textFile(file)\n",
    "    rawHeader = rddFile.first()\n",
    "    header = rawHeader.split(\",\")\n",
    "    rawRows = rddFile.filter(lambda l : l != rawHeader)\n",
    "    res = dict()\n",
    "    for col in header :\n",
    "        res[col] = rawRows.map(lambda l : loadRecord(l, header, col)).sortByKey()\n",
    "    return res\n",
    "\n",
    "def arbreKDsparkRec (sortedData, colList, curCol) :\n",
    "    ### Implementer\n",
    "    return \"\"\n",
    "\n",
    "def arbreKDspark (inputFile, outputFile) :\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print(\"Constitution d'un arbre KD avec Spark\")\n",
    "    print(\"Entrée : \" + inputFile + \" - Sortie : \" + outputFile)\n",
    "    startTime = time.time()\n",
    "    \n",
    "    sc = pyspark.SparkContext(appName=\"arbreKD\")\n",
    "    \n",
    "    rddDict = createRddDict(sc, inputFile)\n",
    "    colList = list(rddDict.keys())\n",
    "    res = arbreKDsparkRec(rddDict, colList, 0)\n",
    "    \n",
    "    sc.stop()\n",
    "    \n",
    "    printSpendTime(startTime)\n",
    "    return res\n",
    "\n",
    "arbre = arbreKDspark(\"data.csv\", \"arbre\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100\n",
      "[{'X': '11.9264091132209', 'Y': '57.1213151095435'}, {'X': '66.3763535907492', 'Y': '50.5452962825075'}, {'X': '92.697916016914', 'Y': '0.581420445814729'}, {'X': '9.23256054520607', 'Y': '75.2423814730719'}, {'X': '29.2442676145583', 'Y': '49.9356162501499'}, {'X': '34.9739615339786', 'Y': '28.3505846280605'}, {'X': '73.1766821118072', 'Y': '69.346927292645'}, {'X': '27.1010003052652', 'Y': '51.4135136036202'}, {'X': '74.4333170354366', 'Y': '70.2033943729475'}, {'X': '59.8525910638273', 'Y': '60.7070606900379'}]\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h00m04s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "### Chargement du RDD (dans un fichier CSV)\n",
    "# import csv\n",
    "# import StringIO\n",
    "\n",
    "import pyspark\n",
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Essai\")\n",
    "dataFile = \"C:/Users/carle/OneDrive/Documents/GitHub/ProjetElementLogiciels/dataFile.csv\"\n",
    "\n",
    "def loadRecord(line) :\n",
    "    res = line.split(\",\")\n",
    "    res = {\"X\":res[0],\"Y\":res[1]}\n",
    "    return res\n",
    "\n",
    "rdd = sc.textFile(dataFile).map(loadRecord)\n",
    "print(rdd.count())\n",
    "print(rdd.take(10))\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "printSpendTime(startTime)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n"
     ]
    }
   ],
   "source": [
    "test = (1,{\"x\":1,\"y\":2})\n",
    "print(test[1][\"x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDsparkRec (sortedData, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    sortCol = colList[curCol]\n",
    "    curData = sortedData[sortCol]\n",
    "    nRow = curData.shape[0]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = curData.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    dataNode = curData.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    leftSortedData = dict()\n",
    "    rightSortedData = dict()\n",
    "    for col in colList :\n",
    "        leftSortedData[col] = sortedData[col][sortedData[col][sortCol]<rootNode.data[sortCol]]\n",
    "        rightSortedData[col] = sortedData[col][sortedData[col][sortCol]>rootNode.data[sortCol]]\n",
    "    rootNode.left = arbreKDpretriRec(leftSortedData, colList, nextCol)\n",
    "    rootNode.right = arbreKDpretriRec(rightSortedData, colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDspark (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    sortedData = dict()\n",
    "    for col in colList :\n",
    "        sortedData[col] = data.sort_values(by = col)\n",
    "    curCol = 0\n",
    "    return(arbreKDsparkRec(sortedData, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on built-in function repr in module builtins:\n",
      "\n",
      "repr(obj, /)\n",
      "    Return the canonical string representation of the object.\n",
      "    \n",
      "    For many object types, including most builtins, eval(repr(obj)) == obj.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(repr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57.3574285286\n",
      "57.357428528562714\n"
     ]
    }
   ],
   "source": [
    "print(data[\"X\"][1])\n",
    "print(repr(data[\"X\"][1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X': '29.444807451657095', 'Y': '66.86872788284245'},\n",
       " {'X': '45.173565574844865', 'Y': '25.15052435089181'},\n",
       " {'X': '83.26567848382234', 'Y': '54.5358545818022'},\n",
       " {'X': '56.13155610717935', 'Y': '40.182397178675345'},\n",
       " {'X': '52.31221096817371', 'Y': '3.741651998050566'},\n",
       " {'X': '23.044419715033314', 'Y': '5.770603777422966'},\n",
       " {'X': '1.487048092245702', 'Y': '44.19470454641951'},\n",
       " {'X': '7.246164719807913', 'Y': '15.007920319739043'},\n",
       " {'X': '26.615822471050798', 'Y': '24.66700415557126'},\n",
       " {'X': '75.02430718936269', 'Y': '36.567030693720135'}]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 5 5.0 2\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "def getMedian (rdd_sorted) :\n",
    "    l = rdd.glom().map(len).collect()  # get length of each partition\n",
    "\n",
    "    n = sum(l) #number of elements in all partitions\n",
    "    median = int(n / 2)\n",
    "    if n % 2 == 1 :\n",
    "        median = int((n-1) / 2)\n",
    "    cumsuml = np.cumsum(l) - median\n",
    "    filtre = [x for x in cumsuml if x>=0] #filter partition with element smallers than median\n",
    "    partition_median = len(l)-len(filtre) #partition where the median lye\n",
    "    \n",
    "    return(rdd.glom().collect()[partition_median][filtre[0]]) #return the median element (X : Xvalue, Y : Yvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 5, 10], dtype=int32)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "True is not in list",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-61-803b3e61afed>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcumsum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ml\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m>=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m//\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m!=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m: True is not in list"
     ]
    }
   ],
   "source": [
    "l.index(min((np.cumsum(l)>=(rdd.count()//2))!=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import floor\n",
    "import time\n",
    "\n",
    "def median(rdd_sorted):\n",
    "    \"\"\"Compute the median\n",
    "    :rdd a numeric rdd\n",
    "    \"\"\"\n",
    "    n = rdd_sorted.count() #compute the number of elements of the RDD\n",
    "    h = (n - 1) * 0.5\n",
    "\n",
    "    rddmedian, rddmedianPlusOne = (rdd_sorted.lookup(x)[0] for x in (floor(h)) + np.array([0, 1]))\n",
    "\n",
    "    return(rddmedian + (h - floor(h)) * (rddXPlusOne - rddX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 33, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-58-9a1ad60db69d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 0 in stage 18.0 failed 1 times, most recent failure: Lost task 0.0 in stage 18.0 (TID 33, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.GeneratedMethodAccessor33.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 268, in dump_stream\n    vs = list(itertools.islice(iterator, batch))\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1580, in <lambda>\n    return self.map(lambda x: x[0])\nKeyError: 0\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ResultTask.runTask(ResultTask.scala:87)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "rdd.keys().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'X': '26.615822471050798', 'Y': '24.66700415557126'}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.glom().collect()[1][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'X': '29.444807451657095', 'Y': '66.86872788284245'},\n",
       " {'X': '45.173565574844865', 'Y': '25.15052435089181'},\n",
       " {'X': '83.26567848382234', 'Y': '54.5358545818022'},\n",
       " {'X': '56.13155610717935', 'Y': '40.182397178675345'},\n",
       " {'X': '52.31221096817371', 'Y': '3.741651998050566'},\n",
       " {'X': '23.044419715033314', 'Y': '5.770603777422966'},\n",
       " {'X': '1.487048092245702', 'Y': '44.19470454641951'},\n",
       " {'X': '7.246164719807913', 'Y': '15.007920319739043'},\n",
       " {'X': '26.615822471050798', 'Y': '24.66700415557126'},\n",
       " {'X': '75.02430718936269', 'Y': '36.567030693720135'}]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = (-6,-4,-2,1,2,3,5,8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(-6, -4, -2, 1, 2, 3, 5, 8)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "liste = [x for x in a if x>=0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5, 8]"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "liste"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
