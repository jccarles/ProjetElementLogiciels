{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Elements logiciels pour le traitement des données en grande dimension"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Installation de pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import findspark\n",
    "sparkHome = \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\"\n",
    "findspark.init(sparkHome)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Petit essai pour calculer PI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def printSpendTime (startTime) :\n",
    "    spendTime = time.time() - startTime\n",
    "    m, s = divmod(spendTime, 60)\n",
    "    h, m = divmod(m, 60)\n",
    "    print(\"--------------------------------------------------\")\n",
    "    print (\"-----   Temps écoulé : %dh%02dm%02ds\" % (h, m, s))\n",
    "    print(\"--------------------------------------------------\")\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.14129808\n",
      "--------------------------------------------------\n",
      "-----   Temps écoulé : 0h01m30s\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "import pyspark\n",
    "import random\n",
    "import time\n",
    "\n",
    "startTime = time.time()\n",
    "\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "\n",
    "sc.stop()\n",
    "\n",
    "printSpendTime(startTime)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Arbre k-d classique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Création d'un jeu de données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000632</td>\n",
       "      <td>43.459306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.916810</td>\n",
       "      <td>25.228089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.990678</td>\n",
       "      <td>24.097988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97.617148</td>\n",
       "      <td>47.040317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.118932</td>\n",
       "      <td>40.414341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.700853</td>\n",
       "      <td>18.913091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.770088</td>\n",
       "      <td>41.075775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.365654</td>\n",
       "      <td>16.492687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23.086512</td>\n",
       "      <td>50.818015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65.901625</td>\n",
       "      <td>71.292907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          Y\n",
       "0   0.000632  43.459306\n",
       "1  13.916810  25.228089\n",
       "2  63.990678  24.097988\n",
       "3  97.617148  47.040317\n",
       "4  92.118932  40.414341\n",
       "5  55.700853  18.913091\n",
       "6   0.770088  41.075775\n",
       "7  15.365654  16.492687\n",
       "8  23.086512  50.818015\n",
       "9  65.901625  71.292907"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "\n",
    "n = 10\n",
    "data = pandas.DataFrame(np.random.rand(n,2)*100,  columns = ['X', 'Y'])\n",
    "data.iloc[0:10, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nXpIKZ9BLUuEMekkqnEEvSYX7f4DG7Qvw7pC0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e9af3da90>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(data[['X']], data[['Y']], 'o')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Définition d'une structure de données pour les arbres binaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Classe permettant de definir un arbre binaire simple\n",
    "### Il se constitue d'un noeud racine, de deux sous-arbres\n",
    "### et des donnees correspondant au noeud racine\n",
    "class Tree(object) :\n",
    "    \n",
    "    ## Constructeur simple\n",
    "    def __init__(self) :\n",
    "        self.left = None\n",
    "        self.right = None\n",
    "        self.key = None\n",
    "        self.data = None\n",
    "    \n",
    "    ## Reprensation de l'arbre sous forme de chaine\n",
    "    ## de caracteres\n",
    "    def __str__(self) :\n",
    "        return self.strRec(0)\n",
    "    \n",
    "    ## Represente l'arbre sous forme de chaine de caracteres\n",
    "    ## de facon recursive\n",
    "    def strRec(self, depth) :\n",
    "        line = \"-\" * 3 * depth\n",
    "        res = line + str(self.data)\n",
    "        if self.left :\n",
    "            res += \"\\n\" + self.left.strRec(depth+1)\n",
    "        if self.right :\n",
    "            res += \"\\n\" + self.right.strRec(depth+1)\n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.lines as lines\n",
    "\n",
    "### Trace un arbre k-d (classe 'Tree') de facon recursive\n",
    "def pltTreeRec(ax, tree, keyX, keyY, xLim, yLim) :\n",
    "    if tree :\n",
    "        x, y = tree.data[keyX], tree.data[keyY]\n",
    "        if tree.key == keyX :\n",
    "            pltTreeRec(ax, tree.left, keyX, keyY, (xLim[0], x), yLim)\n",
    "            pltTreeRec(ax, tree.right, keyX, keyY, (x, xLim[1]), yLim)\n",
    "            ax.add_line(lines.Line2D((x, x), yLim, linewidth=0.5, color='red'))\n",
    "        elif tree.key == keyY :\n",
    "            pltTreeRec(ax, tree.left, keyX, keyY, xLim, (yLim[0], y))\n",
    "            pltTreeRec(ax, tree.right, keyX, keyY, xLim, (y, yLim[1]))\n",
    "            ax.add_line(lines.Line2D(xLim, (y, y), linewidth=0.5, color='red'))\n",
    "        plt.plot(x, y, 'o', color='#1f77b4')\n",
    "\n",
    "### Trace un jeu de donnees et l'arbre k-d associe.\n",
    "### Le parametre 'tree' doit etre une instance de la \n",
    "### classe 'Tree'\n",
    "def plotDataTree(data, tree) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    keyX, keyY = colList[0], colList[1]\n",
    "    ax = plt.axes()\n",
    "    plt.plot(data[['X']], data[['Y']], 'o', color='#1f77b4')\n",
    "    xLim, yLim = ax.get_xlim(), ax.get_ylim()\n",
    "    pltTreeRec(ax, tree, keyX, keyY, xLim, yLim)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Algorithme \"naïf\" pour constituer un arbre k-d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>\n",
    "On propose une première implémentation, \"naïve\", pour la constitution d'un arbre k-d. Le principe : on découpe de façon cyclique selon les différentes dimension ; à chaque étape on recherche la médiane du tableau (donc une opération de tri) selon la dimension courante ; on lui attache deux sous-arbres correspondants aux valeurs inférieures (resp. supérieures) à la médiane.\n",
    "</p>\n",
    "<p>\n",
    "D'après Bentley, si on est en mesure de trouver la médiane en $O(n)$, la complexité de l'algorithme est en $O(n\\log(n))$.\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### Fonction recursive pour constituer un arbre k-d\n",
    "def arbreKDnaifRec (data, colList, curCol) :\n",
    "    nCol = len(colList)\n",
    "    nRow = data.shape[0]\n",
    "    sortCol = colList[curCol]\n",
    "    if nRow == 0 :\n",
    "        return None\n",
    "    elif nRow == 1 :\n",
    "        dataNode = data.iloc[0,]\n",
    "        rootNode = Tree()\n",
    "        rootNode.key = sortCol\n",
    "        rootNode.data = dict()\n",
    "        for col in colList :\n",
    "            rootNode.data[col] = dataNode[col].item()\n",
    "        return rootNode\n",
    "    median = int(nRow / 2)\n",
    "    if nRow % 2 == 1 :\n",
    "        median = int((nRow-1) / 2)\n",
    "    nextCol = (curCol + 1) % nCol\n",
    "    #if nextCol > nCol :\n",
    "    #    nextCol = 0\n",
    "    dataSort = data.sort_values(by=sortCol)\n",
    "    dataNode = dataSort.iloc[median,]\n",
    "    rootNode = Tree()\n",
    "    rootNode.key = sortCol\n",
    "    rootNode.data = dict()\n",
    "    for col in colList :\n",
    "        rootNode.data[col] = dataNode[col].item()\n",
    "    rootNode.left = arbreKDnaifRec(dataSort.iloc[0:median,], colList, nextCol)\n",
    "    rootNode.right = arbreKDnaifRec(dataSort.iloc[(median+1):nRow,], colList, nextCol)\n",
    "    return rootNode\n",
    "\n",
    "### Fonction englobante permettant la construction d'un arbre k-d\n",
    "def arbreKDnaif (data) :\n",
    "    colList = data.columns.values.tolist()\n",
    "    curCol = 0\n",
    "    return(arbreKDnaifRec(data, colList, curCol))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'X': 54.75075719536828, 'Y': 63.824244453234}\n",
      "---{'X': 39.7080823609783, 'Y': 37.31175788146609}\n",
      "------{'X': 50.17047332863738, 'Y': 16.883424074944998}\n",
      "---------{'X': 14.47342361960835, 'Y': 33.185257462076514}\n",
      "------{'X': 47.95688534856048, 'Y': 75.62759975519805}\n",
      "---------{'X': 4.993498181535461, 'Y': 38.867207948529426}\n",
      "---{'X': 57.00285995185999, 'Y': 59.88979710104934}\n",
      "------{'X': 92.34796621349169, 'Y': 31.070900413528847}\n",
      "---------{'X': 71.68850105165654, 'Y': 6.846687544526842}\n",
      "------{'X': 98.53685216505845, 'Y': 98.28087181382776}\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x23e9b27ef60>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "### Test de la fonction 'arbreKDnaif' sur le \n",
    "### jeu de donnees 'data'\n",
    "arbre = arbreKDnaif(data)\n",
    "print(arbre)\n",
    "plotDataTree(data, arbre)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-2bcc27c49eb5>:2 ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-207a850c68c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkConf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetMaster\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"local\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msetAppName\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"KdTree\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0msc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls)\u001b[0m\n\u001b[0;32m    113\u001b[0m         \"\"\"\n\u001b[0;32m    114\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_callsite\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfirst_spark_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mCallSite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 115\u001b[1;33m         \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgateway\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mgateway\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconf\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mconf\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    116\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    117\u001b[0m             self._do_init(master, appName, sparkHome, pyFiles, environment, batchSize, serializer,\n",
      "\u001b[1;32mC:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\context.py\u001b[0m in \u001b[0;36m_ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    297\u001b[0m                         \u001b[1;34m\" created by %s at %s:%s \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    298\u001b[0m                         % (currentAppName, currentMaster,\n\u001b[1;32m--> 299\u001b[1;33m                             callsite.function, callsite.file, callsite.linenum))\n\u001b[0m\u001b[0;32m    300\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m                     \u001b[0mSparkContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_active_spark_context\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0minstance\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Cannot run multiple SparkContexts at once; existing SparkContext(app=pyspark-shell, master=local[*]) created by __init__ at <ipython-input-10-2bcc27c49eb5>:2 "
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "'''from pyspark import SparkConf, SparkContext'''\n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"KdTree\")\n",
    "sc = SparkContext(conf = conf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10, 2)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "sc = pyspark.SparkContext()\n",
    "data_par = sc.parallelize(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://192.168.1.43:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.2.0</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>pyspark-shell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=pyspark-shell>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted1 = data_par.sortByKey()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ParallelCollectionRDD[0] at parallelize at PythonRDD.scala:480"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_par"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['X', 'Y']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_par.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 23, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1752, in add_shuffle_key\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1752, in add_shuffle_key\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-7cd5aadd3cf3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msorted1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\u001b[0m in \u001b[0;36mcollect\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    807\u001b[0m         \"\"\"\n\u001b[0;32m    808\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 809\u001b[1;33m             \u001b[0mport\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jvm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPythonRDD\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcollectAndServe\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrdd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    810\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mport\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jrdd_deserializer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    811\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1131\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1132\u001b[0m         return_value = get_return_value(\n\u001b[1;32m-> 1133\u001b[1;33m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0m\u001b[0;32m   1134\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1135\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mtemp_arg\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mtemp_args\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\py4j-0.10.4-src.zip\\py4j\\protocol.py\u001b[0m in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    317\u001b[0m                 raise Py4JJavaError(\n\u001b[0;32m    318\u001b[0m                     \u001b[1;34m\"An error occurred while calling {0}{1}{2}.\\n\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 319\u001b[1;33m                     format(target_id, \".\", name), value)\n\u001b[0m\u001b[0;32m    320\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    321\u001b[0m                 raise Py4JError(\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling z:org.apache.spark.api.python.PythonRDD.collectAndServe.\n: org.apache.spark.SparkException: Job aborted due to stage failure: Task 3 in stage 5.0 failed 1 times, most recent failure: Lost task 3.0 in stage 5.0 (TID 23, localhost, executor driver): org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1752, in add_shuffle_key\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\tat java.lang.Thread.run(Unknown Source)\r\n\nDriver stacktrace:\r\n\tat org.apache.spark.scheduler.DAGScheduler.org$apache$spark$scheduler$DAGScheduler$$failJobAndIndependentStages(DAGScheduler.scala:1499)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1487)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$abortStage$1.apply(DAGScheduler.scala:1486)\r\n\tat scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:59)\r\n\tat scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.abortStage(DAGScheduler.scala:1486)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGScheduler$$anonfun$handleTaskSetFailed$1.apply(DAGScheduler.scala:814)\r\n\tat scala.Option.foreach(Option.scala:257)\r\n\tat org.apache.spark.scheduler.DAGScheduler.handleTaskSetFailed(DAGScheduler.scala:814)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.doOnReceive(DAGScheduler.scala:1714)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1669)\r\n\tat org.apache.spark.scheduler.DAGSchedulerEventProcessLoop.onReceive(DAGScheduler.scala:1658)\r\n\tat org.apache.spark.util.EventLoop$$anon$1.run(EventLoop.scala:48)\r\n\tat org.apache.spark.scheduler.DAGScheduler.runJob(DAGScheduler.scala:630)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2022)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2043)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2062)\r\n\tat org.apache.spark.SparkContext.runJob(SparkContext.scala:2087)\r\n\tat org.apache.spark.rdd.RDD$$anonfun$collect$1.apply(RDD.scala:936)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)\r\n\tat org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)\r\n\tat org.apache.spark.rdd.RDD.withScope(RDD.scala:362)\r\n\tat org.apache.spark.rdd.RDD.collect(RDD.scala:935)\r\n\tat org.apache.spark.api.python.PythonRDD$.collectAndServe(PythonRDD.scala:458)\r\n\tat org.apache.spark.api.python.PythonRDD.collectAndServe(PythonRDD.scala)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)\r\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)\r\n\tat java.lang.reflect.Method.invoke(Unknown Source)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:280)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.GatewayConnection.run(GatewayConnection.java:214)\r\n\tat java.lang.Thread.run(Unknown Source)\r\nCaused by: org.apache.spark.api.python.PythonException: Traceback (most recent call last):\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 177, in main\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\worker.py\", line 172, in process\n  File \"C:\\Users\\carle\\spark\\spark-2.2.0-bin-hadoop2.7\\python\\lib\\pyspark.zip\\pyspark\\serializers.py\", line 138, in dump_stream\n    for obj in iterator:\n  File \"C:/Users/carle/spark/spark-2.2.0-bin-hadoop2.7\\python\\pyspark\\rdd.py\", line 1752, in add_shuffle_key\n    for k, v in iterator:\nValueError: not enough values to unpack (expected 2, got 1)\n\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.read(PythonRDD.scala:193)\r\n\tat org.apache.spark.api.python.PythonRunner$$anon$1.<init>(PythonRDD.scala:234)\r\n\tat org.apache.spark.api.python.PythonRunner.compute(PythonRDD.scala:152)\r\n\tat org.apache.spark.api.python.PythonRDD.compute(PythonRDD.scala:63)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.api.python.PairwiseRDD.compute(PythonRDD.scala:395)\r\n\tat org.apache.spark.rdd.RDD.computeOrReadCheckpoint(RDD.scala:323)\r\n\tat org.apache.spark.rdd.RDD.iterator(RDD.scala:287)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:96)\r\n\tat org.apache.spark.scheduler.ShuffleMapTask.runTask(ShuffleMapTask.scala:53)\r\n\tat org.apache.spark.scheduler.Task.run(Task.scala:108)\r\n\tat org.apache.spark.executor.Executor$TaskRunner.run(Executor.scala:335)\r\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)\r\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)\r\n\t... 1 more\r\n"
     ]
    }
   ],
   "source": [
    "sorted1.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>X</th>\n",
       "      <th>Y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000632</td>\n",
       "      <td>43.459306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.916810</td>\n",
       "      <td>25.228089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>63.990678</td>\n",
       "      <td>24.097988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>97.617148</td>\n",
       "      <td>47.040317</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>92.118932</td>\n",
       "      <td>40.414341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>55.700853</td>\n",
       "      <td>18.913091</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.770088</td>\n",
       "      <td>41.075775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>15.365654</td>\n",
       "      <td>16.492687</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>23.086512</td>\n",
       "      <td>50.818015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>65.901625</td>\n",
       "      <td>71.292907</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           X          Y\n",
       "0   0.000632  43.459306\n",
       "1  13.916810  25.228089\n",
       "2  63.990678  24.097988\n",
       "3  97.617148  47.040317\n",
       "4  92.118932  40.414341\n",
       "5  55.700853  18.913091\n",
       "6   0.770088  41.075775\n",
       "7  15.365654  16.492687\n",
       "8  23.086512  50.818015\n",
       "9  65.901625  71.292907"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "sqlContext = SQLContext(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------------+\n",
      "|                   X|                 Y|\n",
      "+--------------------+------------------+\n",
      "|6.319045131264467E-4| 43.45930551102589|\n",
      "|  13.916809584905021|25.228088913964687|\n",
      "|  63.990678246739854|24.097987666892053|\n",
      "|   97.61714816747865|47.040316563660625|\n",
      "|   92.11893199989487| 40.41434070229237|\n",
      "|   55.70085277293438| 18.91309129477413|\n",
      "|  0.7700882334788273| 41.07577457016743|\n",
      "|  15.365654361675407| 16.49268733877166|\n",
      "|  23.086512393459945| 50.81801465007638|\n",
      "|   65.90162534569434| 71.29290722528016|\n",
      "+--------------------+------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spDF = sqlContext.createDataFrame(data)\n",
    "spDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_par = spDF.rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[43.45930551102589,\n",
       " 25.228088913964687,\n",
       " 24.097987666892053,\n",
       " 47.040316563660625,\n",
       " 40.41434070229237,\n",
       " 18.91309129477413,\n",
       " 41.07577457016743,\n",
       " 16.49268733877166,\n",
       " 50.81801465007638,\n",
       " 71.29290722528016]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_par.values().collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
